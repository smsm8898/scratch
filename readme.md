# Scratch
Understanding and Implementing the fundamental building blocks of Deep Learning from original papers

## Prerequisities
- python >= 3.9
- numpy >= 2.0.0
- scikit-learn >= 1.6.0

## Experiment
- To check the functions, open `activations`, `optimizers`, `losses`
- For `Augmentation` experiment, I used `torchvision.transforms` functions
- The experiment is implemented on `notebooks`

---
<br>

## ğŸ”¸ [Activation Functions](https://velog.io/@smsm8898/Study-Activation-Functions)

| No | Name                                  | PyTorch  | íŠ¹ì§•                  | 
| -- | ------------------------------------- | -------------- | -------------------------- | 
| 1  | **Sigmoid**                           | `torch.nn.Sigmoid`   | ì¶œë ¥ 0~1 / ì´ì§„ ë¶„ë¥˜ ì‹œ ì‚¬ìš©        |
| 2  | **Tanh**                              | `torch.nn.Tanh`      | ì¶œë ¥ -1~1 / ì¤‘ì‹¬í™”ëœ í™œì„±í™”         |
| 3  | **ReLU**                              | `torch.nn.ReLU`      | ì–‘ìˆ˜ë§Œ í†µê³¼ / sparse activation | 
| 4  | **Leaky ReLU**                        | `torch.nn.LeakyReLU` | ìŒìˆ˜ ì˜ì—­ë„ ì‘ì€ ê¸°ìš¸ê¸° ìœ ì§€           | 
| 5  | **ELU (Exponential Linear Unit)**     | `torch.nn.ELU`       | ìŒìˆ˜ ì˜ì—­ ì™„ë§Œ / í‰ê·  í™œì„±í™” 0 ê·¼ì²˜     |
| 6  | **GELU (Gaussian Error Linear Unit)** | `torch.nn.GELU`      | í™•ë¥ ì  / Transformer ê³„ì—´ì—ì„œ ì‚¬ìš©  |


---
<br>

## ğŸ”¸ [Optimizers Functions](https://velog.io/@smsm8898/Study-Optimizer-Functions)

| No | Name                                    | PyTorch        | íŠ¹ì§•                | 
| -- | --------------------------------------- | -------------------------------- | ------------------------ | 
| 1  | **SGD**                                 | `torch.optim.SGD`                | ê¸°ë³¸ í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•            |
| 2  | **SGD + Momentum**                      | `torch.optim.SGD(momentum=0.9)`  | ëª¨ë©˜í…€ ì ìš© SGD               | 
| 3  | **Nesterov Accelerated Gradient (NAG)** | `torch.optim.SGD(nesterov=True)` | NAG / lookahead gradient | 
| 4  | **AdaGrad**                             | `torch.optim.Adagrad`            | í•™ìŠµë¥  ì ì‘ / ê³¼ê±° gradient ë°˜ì˜  |
| 5  | **RMSProp**                             | `torch.optim.RMSprop`            | ì§€ìˆ˜ ì´ë™ í‰ê·  ê¸°ë°˜ í•™ìŠµë¥  ì¡°ì ˆ       |
| 6  | **Adam**                                | `torch.optim.Adam`               | Momentum + RMSProp ê²°í•©    | 
| 7  | **AdamW**                               | `torch.optim.AdamW`              | Weight Decay ë¶„ë¦¬ ì ìš©       |


---
<br>

## ğŸ”¸ [Loss Functions](https://velog.io/@smsm8898/Study-Loss-Functions)

| No | Name                                  | PyTorch            | Type                         | 
| -- | ------------------------------------- | ------------------------ | ---------------------------- | 
| 1  | **Mean Squared Error Loss**           | `torch.nn.MSELoss`             | Regression                   |
| 2  | **Mean Absolute Error Loss / L1Loss** | `torch.nn.L1Loss`              | Regression                   |
| 3  | **Huber Loss / Smooth L1 Loss**       | `torch.nn.SmoothL1Loss`        | Regression / Robust          |
| 4  | **Binary Cross Entropy Loss**         | `torch.nn.BCELoss`             | Binary Classification        |
| 5  | **Cross Entropy Loss**                | `torch.nn.CrossEntropyLoss`    | Multi-class Classification   |


---
<br>

## ğŸ”¸ [Augmentation](https://velog.io/@smsm8898/Study-Augmentationvision)

| No | Augmentation | ì •ì˜ | ë™ì‘ ì›ë¦¬ | í•™ìŠµ íš¨ê³¼ |
|----|--------------|------|-----------|-----------|
| 1  | **RandomHorizontalFlip** | ì´ë¯¸ì§€ë¥¼ ì¢Œìš°ë¡œ ë’¤ì§‘ëŠ” ì¦ê°• | í™•ë¥  `p`ì— ë”°ë¼ ì¢Œìš° ë°˜ì „ | ì¢Œìš° ëŒ€ì¹­ ê°ì²´ë‚˜ ë°©í–¥ ë¯¼ê°ë„ ê°ì†Œ, ê°•ê±´í•œ í•™ìŠµ |
| 2  | **RandomResizedCrop / CenterCrop** | ì´ë¯¸ì§€ ì¼ë¶€ ì˜ì—­ ì„ íƒ í›„ ì§€ì • í¬ê¸°ë¡œ ì¡°ì • | ëœë¤/ì¤‘ì•™ ìœ„ì¹˜ ì„ íƒ â†’ crop â†’ resize | ë‹¤ì–‘í•œ ìœ„ì¹˜ í•™ìŠµ, ìœ„ì¹˜ ë³€í™”ì— ê°•ê±´ |
| 3  | **ColorJitter** | ë°ê¸°, ëŒ€ë¹„, ì±„ë„, ìƒ‰ì¡° ëœë¤ ë³€í™˜ | ì§€ì • ë²”ìœ„ ë‚´ ìš”ì†Œ ë¬´ì‘ìœ„ ì¡°ì • | ì¡°ëª…/ìƒ‰ìƒ ë³€í™”ì— ê°•ê±´í•œ í•™ìŠµ |
| 4  | **RandomRotation** | ì´ë¯¸ì§€ ì§€ì • ê°ë„ ë²”ìœ„ ë‚´ íšŒì „ | ëœë¤ ê°ë„ ì„ íƒ â†’ ì´ë¯¸ì§€ íšŒì „, í•„ìš”ì‹œ padding | ë°©í–¥ ë³€í™”ì— ê°•ê±´, íšŒì „ ë¶ˆë³€ íŠ¹ì§• í•™ìŠµ |
| 5  | **RandomAffine** | íšŒì „, ì´ë™, ìŠ¤ì¼€ì¼, ì „ë‹¨ ë“± ë³µí•© ë³€í˜• | ì§€ì • ë²”ìœ„ ë‚´ ë³€í˜• ì ìš© | ìœ„ì¹˜, í¬ê¸°, í˜•íƒœ ë³€í™”ì— ê°•ê±´ |
| 6  | **RandomGrayscale** | ì¼ì • í™•ë¥ ë¡œ ì´ë¯¸ì§€ í‘ë°± ë³€í™˜ | í™•ë¥  `p`ë¡œ ë³€í™˜ â†’ 3ì±„ë„ ìœ ì§€ | ìƒ‰ìƒ ì˜ì¡´ë„ ê°ì†Œ, êµ¬ì¡°ì  íŠ¹ì§• í•™ìŠµ ê°•í™” |
| 7  | **GaussianBlur** | ê°€ìš°ì‹œì•ˆ í•„í„°ë¡œ íë¦¼ ì²˜ë¦¬ | ì»¤ë„ í¬ê¸° & í‘œì¤€í¸ì°¨ â†’ ì´ë¯¸ì§€ convolution | ë””í…Œì¼ ë³€í™”, ë…¸ì´ì¦ˆ ê°•ê±´ì„± í–¥ìƒ |
| 8  | **RandomErasing / Cutout** | ì´ë¯¸ì§€ ì¼ë¶€ ì˜ì—­ ì§€ìš°ê¸°/ê°€ë¦¼ | ëœë¤ ìœ„ì¹˜ & í¬ê¸° ì„ íƒ â†’ í”½ì…€ 0 ë˜ëŠ” ì§€ì •ê°’ ì±„ì›€ | Occlusion ëŒ€ì‘, ì¼ë¶€ ì •ë³´ ì†ì‹¤ì—ë„ ê°•ê±´ |
| 9  | **Mixup** | ë‘ ì´ë¯¸ì§€ + ë¼ë²¨ì„ ê°€ì¤‘ í‰ê· í•˜ì—¬ í•©ì„± | Î» ìƒ˜í”Œë§ â†’ `x_new = Î»*x1 + (1-Î»)*x2`, `y_new = Î»*y1 + (1-Î»)*y2` | ê²½ê³„ ë¯¼ê°ë„ ê°ì†Œ, ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ, ì˜¤ë²„í”¼íŒ… ê°ì†Œ |
| 10 | **CutMix** | ì´ë¯¸ì§€ ì¼ë¶€ ì˜ì—­ êµì²´ + ë¼ë²¨ í˜¼í•© | x2 ì¼ë¶€ ì˜ì—­ x1ì— ë®ì–´ì“°ê¸° â†’ Î» = íŒ¨ì¹˜ ë¹„ìœ¨ â†’ y_new í˜¼í•© | Occlusion ëŒ€ì‘, Mixupë³´ë‹¤ í˜„ì‹¤ì  í˜¼í•©, ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ |


---
<br>

## ğŸ”¸ [Scheduler](https://velog.io/@smsm8898/Study-Learning-Scheduler)

| No | Scheduler             | ì •ì˜                                       | ë™ì‘ ì›ë¦¬                                              | í•™ìŠµ íš¨ê³¼                                                     |
| -- | --------------------- | ---------------------------------------- | -------------------------------------------------- | --------------------------------------------------------- |
| 1  | **StepLR**            | ì¼ì • epoch ê°„ê²©ë§ˆë‹¤ lrì„ ê°ì†Œì‹œí‚¤ëŠ” ìŠ¤ì¼€ì¤„ëŸ¬             | `step_size` epochë§ˆë‹¤ `lr = lr * gamma`              | ì•ˆì •ì ì¸ ìˆ˜ë ´, ê°„ë‹¨í•˜ê³  ê¸°ë³¸ì ì¸ lr ìŠ¤ì¼€ì¤„ë§                                |
| 2  | **CosineAnnealingLR** | lrì„ cosine ê³¡ì„  í˜•íƒœë¡œ ë¶€ë“œëŸ½ê²Œ ê°ì†Œ                | `T_max` epoch ë™ì•ˆ lrì´ ìµœëŒ€ â†’ ìµœì†Œ(`eta_min`)ë¡œ ê°ì†Œ        | ë¶€ë“œëŸ¬ìš´ lr ê°ì†Œë¡œ ëª¨ë¸ì´ ì„¸ë°€í•˜ê²Œ ìˆ˜ë ´, ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ                        |
| 3  | **ReduceLROnPlateau** | val lossê°€ ê°œì„ ë˜ì§€ ì•Šì„ ë•Œ lr ê°ì†Œ                | `patience` epoch ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ `lr = lr * factor`      | ìë™ lr ì¡°ì •ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ ê°ì†Œ ë°©ì§€, fine-tuningê³¼ transfer learningì— ìœ ë¦¬ |
| 4  | **OneCycleLR**        | lrì„ í•œ ì£¼ê¸° ë™ì•ˆ ì˜¬ë ¸ë‹¤ê°€ ë‚´ë¦¬ëŠ” ë°©ì‹, momentum ë°˜ëŒ€ë¡œ ì¡°ì ˆ | ì´ˆê¸° lr â†’ `max_lr`ë¡œ ì¦ê°€ â†’ ë‹¤ì‹œ ê°ì†Œ, momentumì€ lrê³¼ ë°˜ëŒ€ë¡œ ë³€í•¨ | ë¹ ë¥¸ íƒìƒ‰ê³¼ ì•ˆì •ì  ìˆ˜ë ´, ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ, ìµœì‹  CNN/Transformer í•™ìŠµì—ì„œ ìì£¼ ì‚¬ìš©   |
