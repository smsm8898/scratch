# Scratch
Understanding and Implementing the fundamental building blocks of Deep Learning from original papers

## Prerequisities
- python >= 3.9
- numpy >= 2.0.0
- scikit-learn >= 1.6.0

---
## ğŸ”¸ [Activation Functions](https://velog.io/@smsm8898/Study-Activation-Functions)

| No | Name                                  | PyTorch  | íŠ¹ì§•                  | 
| -- | ------------------------------------- | -------------- | -------------------------- | 
| 1  | **Sigmoid**                           | `torch.nn.Sigmoid`   | ì¶œë ¥ 0~1 / ì´ì§„ ë¶„ë¥˜ ì‹œ ì‚¬ìš©        |
| 2  | **Tanh**                              | `torch.nn.Tanh`      | ì¶œë ¥ -1~1 / ì¤‘ì‹¬í™”ëœ í™œì„±í™”         |
| 3  | **ReLU**                              | `torch.nn.ReLU`      | ì–‘ìˆ˜ë§Œ í†µê³¼ / sparse activation | 
| 4  | **Leaky ReLU**                        | `torch.nn.LeakyReLU` | ìŒìˆ˜ ì˜ì—­ë„ ì‘ì€ ê¸°ìš¸ê¸° ìœ ì§€           | 
| 5  | **ELU (Exponential Linear Unit)**     | `torch.nn.ELU`       | ìŒìˆ˜ ì˜ì—­ ì™„ë§Œ / í‰ê·  í™œì„±í™” 0 ê·¼ì²˜     |
| 6  | **GELU (Gaussian Error Linear Unit)** | `torch.nn.GELU`      | í™•ë¥ ì  / Transformer ê³„ì—´ì—ì„œ ì‚¬ìš©  |


---
## ğŸ”¸ [Optimizers Functions](https://velog.io/@smsm8898/Study-Optimizer-Functions)

| No | Name                                    | PyTorch        | íŠ¹ì§•                | 
| -- | --------------------------------------- | -------------------------------- | ------------------------ | 
| 1  | **SGD**                                 | `torch.optim.SGD`                | ê¸°ë³¸ í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•            |
| 2  | **SGD + Momentum**                      | `torch.optim.SGD(momentum=0.9)`  | ëª¨ë©˜í…€ ì ìš© SGD               | 
| 3  | **Nesterov Accelerated Gradient (NAG)** | `torch.optim.SGD(nesterov=True)` | NAG / lookahead gradient | 
| 4  | **AdaGrad**                             | `torch.optim.Adagrad`            | í•™ìŠµë¥  ì ì‘ / ê³¼ê±° gradient ë°˜ì˜  |
| 5  | **RMSProp**                             | `torch.optim.RMSprop`            | ì§€ìˆ˜ ì´ë™ í‰ê·  ê¸°ë°˜ í•™ìŠµë¥  ì¡°ì ˆ       |
| 6  | **Adam**                                | `torch.optim.Adam`               | Momentum + RMSProp ê²°í•©    | 
| 7  | **AdamW**                               | `torch.optim.AdamW`              | Weight Decay ë¶„ë¦¬ ì ìš©       |



## ğŸ”¸ Loss Functions
| No | Name                                  | PyTorch            | Type                         | 
| -- | ------------------------------------- | ------------------------ | ---------------------------- | 
| 1  | **Mean Squared Error Loss**           | `torch.nn.MSELoss`             | Regression                   |
| 2  | **Mean Absolute Error Loss / L1Loss** | `torch.nn.L1Loss`              | Regression                   |
| 3  | **Binary Cross Entropy Loss**         | `torch.nn.BCELoss`             | Binary Classification        |
| 4  | **Binary Cross Entropy with Logits**  | `torch.nn.BCEWithLogitsLoss`   | Binary Classification        |
| 5  | **Cross Entropy Loss**                | `torch.nn.CrossEntropyLoss`    | Multi-class Classification   |
| 6  | **Huber Loss / Smooth L1 Loss**       | `torch.nn.SmoothL1Loss`        | Regression / Robust          |
| 7  | **KL Divergence Loss**                | `torch.nn.KLDivLoss`           | Distribution / Probabilities |
| 8  | **Hinge Loss (Multi-margin)**         | `torch.nn.MultiMarginLoss`     | Classification               |
| 9  | **Cosine Embedding Loss**             | `torch.nn.CosineEmbeddingLoss` | Metric Learning              |
| 10 | **Triplet Margin Loss**               | `torch.nn.TripletMarginLoss`   | Metric Learning              |

